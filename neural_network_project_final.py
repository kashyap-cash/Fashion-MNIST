# -*- coding: utf-8 -*-
"""Neural Network Project Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ixelqBMauwJfN33TGJQlG6_9h5IV_EJG
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf # Import tensorflow library
from tensorflow import keras # Import Keras Library

"""# 3. Import Data"""

fashion_mnist = keras.datasets.fashion_mnist
(X_train,Y_train),(X_test,Y_test)=fashion_mnist.load_data() # Import data from keras server

print("x_train shape:", X_train.shape, "y_train shape:", Y_train.shape)

X_train=X_train.reshape(60000,784) #Reshape the image pixel of 28x28 to 784

print(X_train.shape)

class_names={0:"T-shirt/top",1: "Trouser",2: "Pullover",3: "Dress",4: "Coat",5: "Sandal",
    6: "Shirt",7: "Sneaker",8: "Bag",9: "Ankle boot"}
print("The Output relation with fashion is: ",class_names) #The output defined in terms of Fashion items

"""# 4. Visulaize the input data"""

plt.imshow(X_train[10].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis("off")
plt.show()

"""# 4.(a) Distribution of training data"""

digit_train, counts_train = np.unique(Y_train, return_counts = True)

plt.bar(digit_train,counts_train,width =0.6)
plt.title('Distribution of Y_train')
plt.xlabel('Digit Number')
plt.ylabel('Counts')
plt.show()

"""Equal distribution of training data

# 4.(b) Distribution of test data
"""

digit_test, counts_test = np.unique(Y_test, return_counts = True)

plt.bar(digit_test,counts_test,width =0.6)
plt.title('Distribution of Y_test')
plt.xlabel('Digit Number')
plt.ylabel('Counts')
plt.show()

"""Equal distribution of test data

# 5. Feature Scaling or Standardization
"""

# Using Standardization Scaler method
from sklearn.preprocessing import StandardScaler 
scaler=StandardScaler()
#from sklearn.preprocessing import MinMaxScaler
#scaler=MinMaxScaler()
X_train_scaled=scaler.fit_transform(X_train)
X_test_scaled=scaler.transform(X_test.reshape(10000,784))

fig,ax = plt.subplots(1,2)
ax[0].imshow(X_train[10].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis("off")
ax[0].set_title('Unscaled')
ax[1].imshow(X_train_scaled[10].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis("off")
ax[1].set_title('Scaled')
plt.show()

fig_object, ax_object = plt.subplots(1, 10, figsize=(12,5))
ax_object = ax_object.reshape(10,)
    
for i in range(len(ax_object)):
    ax = ax_object[i]
    idx=np.argwhere(Y_train==i)[0]
    ax.imshow(X_train[idx].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest')
    ax.set_xlabel(class_names[int(Y_train[idx])])
    ax.set_title(i)
    
plt.show()

fig_object, ax_object = plt.subplots(1, 10, figsize=(12,5))
ax_object = ax_object.reshape(10,)
      
for i in range(len(ax_object)):
    ax = ax_object[i]
    idx=np.argwhere(Y_train==i)[0]
    ax.imshow(X_train_scaled[idx].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest')
    ax.set_xlabel(class_names[int(Y_train[idx])])
    plt.xlabel(class_names[int(Y_train[idx])])
    ax.set_title(i)       
plt.show()

"""# 6. Building Model"""

import keras
from keras.models import Sequential
from keras.layers import Dense
#Import libraries to build the model

"""# 6.1.1 Training Accuracy with change in no. of Neurons"""

a=[1, 60, 200,3000]
test_accu=np.zeros(4)
train_accu=np.zeros(4)
plt.figure(figsize=(8,6))
for i in range(4):
    model=Sequential()
    model.add(Dense(output_dim=a[i],init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 1024, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    plt.plot(range(epoch),history.history['acc'],label='No. of Neurons:'+str(a[i]))
    train_accu[i]=history.history['acc'][epoch-1]
plt.title('Training Accuracy')
plt.xlabel('No. of Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.show()
plt.figure(figsize=(8,6))
plt.plot(a,test_accu,label='Test Accuracy')
plt.xlabel('Complexity')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.plot(a,train_accu,label='Train Accuracy')
plt.legend()
plt.show()

"""# 6.1.2 Increase in Number of Hidden Layer"""

a=[1, 2, 5]
test_accu=np.zeros(4)
train_accu=np.zeros(4)
plt.figure(figsize=(8,6))
for i in range(len(a)):
    model=Sequential()
    for j in range(a[i]):
        model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    plt.plot(range(epoch),history.history['acc'],label='No. of Hidden Layers'+str(a[i]))
    train_accu[i]=history.history['acc'][epoch-1]
plt.title('Training Accuracy')
plt.xlabel('No. of Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
plt.plot(a,test_accu[:-1],label='Test Accuracy')
plt.xlabel('Complexity')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.plot(a,train_accu[:-1],label='Train Accuracy')
plt.legend()
plt.show()

"""# 6.1.3 Change in Batch Size & Epochs"""

a=[64, 128, 1024, 4096]
b=[5, 8, 10, 12]
test_accu=np.zeros(4)
train_accu=np.zeros(4)
plt.figure(figsize=(8,6))
for i in range(4):
    model=Sequential()
    model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=b[i]
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= a[i], epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    plt.plot(range(epoch),history.history['acc'],label='No. of Batch Size:'+str(a[i]))
    train_accu[i]=history.history['acc'][epoch-1]
plt.title('Training Accuracy')
plt.xlabel('No. of Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.show()
plt.figure(figsize=(8,6))
plt.plot(a,test_accu,label='Test Accuracy')
plt.xlabel('Batch Size')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.plot(a,train_accu,label='Train Accuracy')
plt.legend()
plt.show()

"""# 6.2.1 Change in Activation Function of Hidden Layer"""

a=['linear','tanh','exponential','hard_sigmoid','sigmoid','relu','softsign','softplus']
test_accu=np.zeros(len(a))
train_accu=np.zeros(len(a))
plt.figure(figsize=(8,6))
for i in range(len(a)):
    model=Sequential()
    model.add(Dense(output_dim=30,init='uniform',activation=a[i],input_dim=784))
    model.add(Dense(output_dim=30,init='uniform',activation=a[i],input_dim=784))
    model.add(Dense(output_dim=30,init='uniform',activation=a[i],input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    train_accu[i]=history.history['acc'][epoch-1]

z1=[0, 1, 2, 3, 4, 5, 6, 7]
z2=[0, 1, 2, 3, 4, 5, 6, 7]
for i in range(len(z1)):
    z1[i]=z1[i]-0.2
    z2[i]=z2[i]+0.2
plt.figure(figsize=(14,8))
plt.bar(z1,test_accu,label='Test Accuracy',width=0.4)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.bar(z2,train_accu,label='Train Accuracy',width=0.4)
plt.xticks([r for r in range(len(z1))],a)
plt.legend()
plt.ylim(0,1)
plt.show()

"""# 6.2.2 Change in Activation Function of Output Layer"""

a=['tanh','hard_sigmoid','sigmoid','softsign','softplus','softmax']
test_accu=np.zeros(len(a))
train_accu=np.zeros(len(a))
plt.figure(figsize=(8,6))
for i in range(len(a)):
    model=Sequential()
    model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation=a[i]))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    train_accu[i]=history.history['acc'][epoch-1]

z1=[0, 1, 2, 3, 4, 5]
z2=[0, 1, 2, 3, 4, 5]
for i in range(len(z1)):
    z1[i]=z1[i]-0.2
    z2[i]=z2[i]+0.2
plt.figure(figsize=(14,8))
plt.bar(z1,test_accu,label='Test Accuracy',width=0.4)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.bar(z2,train_accu,label='Train Accuracy',width=0.4)
plt.xticks([r for r in range(len(z1))],a)
plt.legend()
plt.ylim(0,1)
plt.show()

"""# 6.3.1 Chane in Optimizer"""

a=['SGD','RMSprop','Adagrad','Adadelta','Adam','Adamax','Nadam']
test_accu=np.zeros(len(a))
train_accu=np.zeros(len(a))
plt.figure(figsize=(8,6))
for i in range(len(a)):
    model=Sequential()
    model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer=a[i],
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    train_accu[i]=history.history['acc'][epoch-1]

z1=[0, 1, 2, 3, 4, 5, 6]
z2=[0, 1, 2, 3, 4, 5,6]
for i in range(len(z1)):
    z1[i]=z1[i]-0.2
    z2[i]=z2[i]+0.2
plt.figure(figsize=(14,8))
plt.bar(z1,test_accu,label='Test Accuracy',width=0.4)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.bar(z2,train_accu,label='Train Accuracy',width=0.4)
plt.xticks([r for r in range(len(z1))],a)
plt.legend()
plt.ylim(0.6,1)
plt.show()

"""# 6.3.2 Chane in Metrics"""

a=['binary_accuracy','accuracy','categorical_accuracy']
test_accu=np.zeros(len(a))
train_accu=np.zeros(len(a))
plt.figure(figsize=(8,6))
for i in range(len(a)):
    model=Sequential()
    model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='Nadam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    epoch=5
    history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)
    test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
    print('\nTest accuracy:', test_acc)
    test_accu[i]=test_acc
    train_accu[i]=history.history['acc'][epoch-1]

z1=[0, 1, 2]
z2=[0, 1, 2]
for i in range(len(z1)):
    z1[i]=z1[i]-0.2
    z2[i]=z2[i]+0.2
plt.figure(figsize=(14,8))
plt.bar(z1,test_accu,label='Test Accuracy',width=0.4)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.bar(z2,train_accu,label='Train Accuracy',width=0.4)
plt.xticks([r for r in range(len(z1))],a)
plt.legend()
plt.ylim(0.85,0.92)
plt.show()

"""# 7 Model Training"""

model=Sequential()

model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))

model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))

"""# 7.1 Compile"""

model.compile(optimizer='Nadam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""# 7.2 Train the model"""

epoch=8

history=model.fit(X_train_scaled.reshape(60000,784), Y_train,batch_size= 128, epochs=epoch)

"""# 7.3 Evaluate the accuracy"""

test_loss, test_acc = model.evaluate(X_test_scaled.reshape(10000,784),  Y_test,verbose=2)
print('\nTest accuracy:', test_acc)

plt.plot(range(epoch),history.history['acc'])
plt.xlabel('No. of Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.show()
plt.plot(range(epoch),history.history['loss'])
plt.xlabel('No. of Epochs')
plt.ylabel('Loss Value')
plt.title('Loss Function')
plt.show()

"""# 7.4 Predictions"""

predict = model.predict(X_test_scaled.reshape(10000,784))

Y_pred=np.zeros(len(predict))
for i in range(len(predict)):
    Y_pred[i]=np.argmax(predict[i])

print(Y_pred)

"""# 8. Cross-Validation"""

def create_model():
    model=Sequential()
    model.add(Dense(output_dim=60,init='uniform',activation='relu',input_dim=784))
    model.add(Dense(output_dim=10,init='uniform',activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    return model

from sklearn.model_selection import cross_val_score
from keras.wrappers.scikit_learn import KerasClassifier
sklearn_model=KerasClassifier(build_fn=create_model,epochs=5,batch_size=110,verbose=0)

accuracies=cross_val_score(sklearn_model,X=X_train_scaled,y=Y_train,cv=10)

plt.bar(range(accuracies.shape[0]),accuracies)
plt.ylabel('Accuracy')
plt.xlabel('K-fold cross validation')
plt.title('Cross Validation')
plt.ylim(0.87,0.89)
plt.show()

"""# 9. Performance Metrics

# 9.1 Confusion Metrics
"""

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test,Y_pred)
print(cm)

"""# 9.1.1 Accuracy"""

a=0
for i in range(len(cm)):
    a=a+cm[i,i]
acc=a/10000
print("Test Accuracy of the Model is:",round(acc,3)*100,"%")

"""# 9.1.2 Precision"""

prec=np.zeros(len(cm))
for j in range(len(cm)):
    precision=0
    for i in range(len(cm)):
        precision= precision + cm[i,j]
    prec[j]=cm[j,j]/precision
#print("Precision of matrix in % is:\n")
a=[0,1,2,3,4,5,6,7,8,9]
print('Output',"\t",'Precision (%)')
for i in range(len(a)):
    print(a[i],"\t",round(prec[i],2)*100)

"""# 9.1.3 Recall"""

recall=np.zeros(len(cm))
for j in range(len(cm)):
    rec=0
    for i in range(len(cm)):
        rec= rec + cm[j,i]
    recall[j]=cm[j,j]/rec

#print("Recall of matrix in % is:\n",recall*100)
a=[0,1,2,3,4,5,6,7,8,9]
print('Output',"\t",'Recall (%)')
for i in range(len(a)):
    print(a[i],"\t",round(recall[i],2)*100)

"""# 9.2 Recall vs Precision"""

count_Y=np.unique(Y_test)
print(count_Y)
fig,ax = plt.subplots(1,2,figsize=(30,10))
ax[0].bar(class_names.values(),recall*100)
ax[0].set_title('Recall', fontsize=20)
ax[1].bar(class_names.values(),prec*100)
ax[1].set_title('Precision',fontsize=20)
plt.show()

plt.figure(figsize=(20,10))
plt.bar(count_Y-0.2,recall*100,width=0.4, color='b', align='center',label='Recall')
plt.bar(count_Y+0.2,prec*100,width=0.4, color='r', align='center', label='Precision')
plt.xticks([r for r in range(len(count_Y))],class_names.values())
plt.legend(loc='best')
plt.title('Recall & Precision Graph',fontsize=20)
plt.ylabel('Percentage (%)''')
plt.show()

